{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import datetime\n",
    "from tqdm.notebook import tqdm \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "import optuna.integration.lightgbm as lgb_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        rawデータ\n",
    "    data_p : pd.DataFrame\n",
    "        preprocessing後のデータ\n",
    "    data_h : pd.DataFrame\n",
    "        merge_horse_results後のデータ\n",
    "    data_pe : pd.DataFrame\n",
    "        merge_peds後のデータ\n",
    "    data_c : pd.DataFrame\n",
    "        process_categorical後のデータ\n",
    "    no_peds: Numpy.array\n",
    "        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.data_p = pd.DataFrame()\n",
    "        self.data_h = pd.DataFrame()\n",
    "        self.data_pe = pd.DataFrame()\n",
    "        self.data_c = pd.DataFrame()\n",
    "        \n",
    "    def merge_horse_results(self, hr, n_sample_list=[5, 9, 'all']):\n",
    "        \n",
    "        \"\"\"\n",
    "        馬の過去成績データから、\n",
    "        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        hr : HorseResults\n",
    "            馬の過去成績データ\n",
    "        n_samples_list : list, default [5, 9, 'all']\n",
    "            過去何レース分追加するか\n",
    "        \"\"\"\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_sanples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "            \n",
    "        #6/6追加　馬の出走感感覚の追加\n",
    "        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days\n",
    "        self.data_h.drop(['開催', 'latest'], axis=1, inplace=True)\n",
    "        \n",
    "    def merge_peds(self, peds):\n",
    "        \"\"\"\n",
    "        5世代分血統データを追加してdata_peに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        peds : Peds.peds_e\n",
    "            Pedsクラスで加工された血統データ。\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_pe = \\\n",
    "            self.data_h.merge(peds, left_on='horse_id', right_index=True,how='left')\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]\\\n",
    "            ['horse_id'].unique()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "            \n",
    "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "        \"\"\"\n",
    "        カテゴリ変数を処理してdata_cに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        le_horse : sklearn.preprocessing.LabelEncoder\n",
    "            horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        le_jockey : sklearn.preprocessing.LabelEncoder\n",
    "            jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        results_m : Results.data_pe\n",
    "            ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n",
    "        \"\"\"\n",
    "        df = self.data_pe.copy()\n",
    "        #ラベルコーディングhorse_id, jockly_idを0始まりの整数に変換\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna(). unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jokcey_id'])\n",
    "        \n",
    "        #horse_id, jockey_idをpandasのカテゴリー型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df[jockey_id].astype('category')\n",
    "        \n",
    "        #その他のカテゴリ変数をpandasのカテゴリ型にしてからダミー変数化\n",
    "        #列を一定にする\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m = ['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_sate'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n",
    "        \n",
    "        self.data_c = df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        rawデータ\n",
    "    data_p : pd.DataFrame\n",
    "        preprocessing後のデータ\n",
    "    data_h : pd.DataFrame\n",
    "        merge_horse_results後のデータ\n",
    "    data_pe : pd.DataFrame\n",
    "        merge_peds後のデータ\n",
    "    data_c : pd.DataFrame\n",
    "        process_categorical後のデータ\n",
    "    no_peds: Numpy.array\n",
    "        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.data_p = pd.DataFrame()\n",
    "        self.data_h = pd.DataFrame()\n",
    "        self.data_pe = pd.DataFrame()\n",
    "        self.data_c = pd.DataFrame()\n",
    "        \n",
    "    def merge_horse_results(self, hr, n_sample_list=[5, 9, all]):\n",
    "        \"\"\"\n",
    "        馬の過去成績データから、\n",
    "        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        hr : HorseResults\n",
    "            馬の過去成績データ\n",
    "        n_samples_list : list, default [5, 9, 'all']\n",
    "            過去何レース分追加するか\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_sample in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "            \n",
    "        #6/6の追加、馬の出馬感覚追加\n",
    "        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days\n",
    "        self.data_h.drop(['開催','latest'], axis=1, inplace=True)\n",
    "        \n",
    "    def merge_peds(self,peds):\n",
    "        \"\"\"\n",
    "        5世代分血統データを追加してdata_peに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        peds : Peds.peds_e\n",
    "            Pedsクラスで加工された血統データ。\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_pe = \\\n",
    "        self.data_h.merge(peds, left_on='horse_id', right_index=True, how='left')\n",
    "        self.no_peds = self.data.data_pe[self_pe['peds_0'].isnull()] ['horse_id'].unique()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds as horse_id_list \"no_peds\"')\n",
    "            \n",
    "        def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "            \"\"\"\n",
    "            カテゴリ変数を処理してdata_cに返す\n",
    "            Parameters:\n",
    "            ----------\n",
    "            le_horse : sklearn.preprocessing.LabelEncoder\n",
    "                horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "            le_jockey : sklearn.preprocessing.LabelEncoder\n",
    "                jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "            results_m : Results.data_pe\n",
    "                ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n",
    "            \"\"\"\n",
    "            \n",
    "            df = self.data_pe.copy()\n",
    "            \n",
    "            #ラベルコーディング。horse_id,jockey_idを0始まりの変数に変換\n",
    "            mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "            new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "            le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "            df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "            mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "            new_jockey_id = df['horse_id'].mask(mask_jockey).dropna().unique()\n",
    "            le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "            df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "            \n",
    "            #horse_id, jockey_idをpandasのカテゴリー型に変換\n",
    "            df['horse_id'] = df['horse_id'].astype('categoty')\n",
    "            df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "            \n",
    "            #その他のカテゴリ変数をpandasのカテゴリ型に変換してからダミー変数化\n",
    "            #列を一定にする\n",
    "            weather = results_m['weather'].unique()\n",
    "            race_types = results_m['race_type'].unique()\n",
    "            ground_states = results_m['ground_states'].unique()\n",
    "            sexes = results_m['性'].unique()\n",
    "            df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "            df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "            df['ground_state'] = pd.Categorical(df['ground_sate'], ground_states)\n",
    "            df['性'] = pd.Categorical(df['性'], sexes)\n",
    "            df = pd.get_dummies(df, ccolumns=['weather', 'race_type', 'ground_sate', '性'])\n",
    "            \n",
    "            self.data_c = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "        \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        \"\"\"\n",
    "        レース結果データをスクレイピングする関数\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "        Returns:\n",
    "        ----------\n",
    "        race_results_df : pandas.DataFrame\n",
    "            全レース結果データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "            \n",
    "        #race_idをkeyにしてdataframe型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race\" + race_id\n",
    "                #メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "                \n",
    "                html = requests.get(url)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "                \n",
    "                #天候、レース種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\" \"data_intro\"}).find_all(\"p\")[0].text + soup.find(\"div\", attrs={\"class\": \"ata_intro\"}).find_all(\"p\")[1].test)\n",
    "                info = re.findall('r\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"course_len\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"雲\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "                        \n",
    "                #馬ID,騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\"a\", attrs={\"href\": re.compile(\"^/horse\")})\n",
    "                \n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\"a\", attrs={\"href\": re.compile(\"^/jockey\")})\n",
    "                \n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "                \n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "                \n",
    "                race_results[race_id] = df\n",
    "                time.sleep(1)\n",
    "                \n",
    "            #存在しないrace_idを飛ばす\n",
    "            except IndexErrot:\n",
    "                continue\n",
    "            #wifiが切れた時にデータを一時的に返す\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            #jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "        #pd.DataFrame型に対して一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "        \n",
    "        return race_results_df\n",
    "    \n",
    "    #前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        #掠順に数位以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
    "        \n",
    "        #性年を性と年齢に分ける\n",
    "        df[\"性\"] = df['性齡'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齡'].map(lambda x: str(x)[1:]).astype(int)\n",
    "                    \n",
    "        #馬体重を体重と体重変化に分ける\n",
    "        df['体重'] = df['馬体重'].str.split(\"(\", expand=True)[0]\n",
    "        df['体重変化'] = df['馬体重'].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "        \n",
    "        #errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
    "        df['体重'] = pd.to_numeric(df['体重'], errors='coerce')\n",
    "        df['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
    "        \n",
    "        #単勝をfloatに変換\n",
    "        df['単勝'] = df['単勝'].astype(float)\n",
    "        #距離は10の値を切り捨てる\n",
    "        df['course_len'] = df[\"course_len\"].astype(float) // 100\n",
    "        \n",
    "        #不要な列を削除\n",
    "        df.drop([\"タイム\",\"着差\",\"調教師\",\"性齡\",\"馬体重\",\"馬名\",\"騎手\",\"人気\",\"着順\"], axis=1, inplace=True)\n",
    "        df['date'] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        \n",
    "        #6/6出走数追加\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "\n",
    "        self.data_p = df\n",
    "    #カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncorder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncorder().fit(self.data_pe['jockey_id'])\n",
    "        super().process_categorical(self.li_horse, self.le_jockey, self.data_pe)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShutubaTable(DataProcessor):\n",
    "    def __init__(self, shutubaTables):\n",
    "        super(ShutubaTable,self).__init__()\n",
    "        self.data = shutuba_tables\n",
    "        \n",
    "    @classmethod\n",
    "    def scrape(cls, race_id, date):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url = url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "            \n",
    "            html = requests.get(url)\n",
    "            html.encording = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                if text in [\"曇\",\"晴\",\"雨\",\"小雨\",\"小雪\",\"雪\"]:\n",
    "                    df[\"weather\"] = [text] * len(df)\n",
    "                if text in [\"良\",\"稍重\",\"量\"]:\n",
    "                    df['ground_state'] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df['ground_state'] = ['不良'] * len(df)\n",
    "                #2020/12/13追加\n",
    "                if '稍' in text:\n",
    "                    df['grpund_state'] = ['稍重'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            df['date'] = [date] * len(df)\n",
    "            \n",
    "            #horse_id\n",
    "            horse_id_list = []\n",
    "            horse_id_list = soup.find_all(\"td\", attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            #jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_id_list = soup.find_all(\"td\", attrs={'class': 'jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['horse_id'] = horse_id_list\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "            \n",
    "            df.index = [race_id] *len(df)\n",
    "            data = data.append(df)\n",
    "            time.sleep(1)\n",
    "        return cls(data)\n",
    "    \n",
    "    #前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df['性'] = df[\"性齡\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齡\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "        \n",
    "        #馬体重を体重と体重変化に分ける\n",
    "        df = df[df[\"馬体重(増減)\"] != '--']\n",
    "        df[\"体重\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "        df[\"体重変化\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "        #2020/12/13追加:増減が前計府などのとき欠損値にする\n",
    "        df['体重変化'] = pd.to_numeric(df['体重変化'],errors='coerce')\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df[\"date\"])\n",
    "        \n",
    "        df['枠'] = df['枠'].astype(int)\n",
    "        df['馬番'] = df['馬番'].astype(int)\n",
    "        df['斤量'] = df['斤量'].astype(int)\n",
    "        \n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        \n",
    "        #6/6出走数追加\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "        \n",
    "        #使用する列を選択\n",
    "        df = df[[\"枠\",\"馬番\",\"斤量\",\"course_len\",\"weather\",\"race_type\",\"groud_state\",\"date\",\"horse_id\",\"jockey_id\",\"性\",\"年齢\",\"体重\",\"体重変化\",\"開催\",\"n_horses\"]]\n",
    "        self.data_p = df.rename(colunbs={'枠': '枠番'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付','着順','賞金','着差','通過','開催','距離']]\n",
    "        self.preprocessing()\n",
    "        \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(horse_id_list):\n",
    "        \"\"\"\n",
    "        馬の過去成績データをスクレイピングする関数\n",
    "        Parameters:\n",
    "        ----------\n",
    "        horse_id_list : list\n",
    "            馬IDのリスト\n",
    "        Returns:\n",
    "        ----------\n",
    "        horse_results_df : pandas.DataFrame\n",
    "            全馬の過去成績データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "        #horse_idをkeyにしてDataFrame型を格納\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "                df = pd.read_html(url)[3]\n",
    "                #受賞歴がある馬の場合、3番目に受賞テーブルがあるため、4番目のデータを取得する\n",
    "                if df.columns[0]=='受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "        \n",
    "        return horse_results_df\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "        \n",
    "        #着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        \n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        #1着の着差を0にする\n",
    "        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n",
    "        \n",
    "        #レース展開データ\n",
    "        #n=1 際車のコーナー位置 n=4 最終コーナー位置\n",
    "        def corner(x, n):\n",
    "            if type(x) != str:\n",
    "                return x\n",
    "            elif n==4:\n",
    "                return int(re.findall(r'\\d+', x)[-1])\n",
    "            elif n==1:\n",
    "                return int(re.findall(r'\\d+', x)[0])\n",
    "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
    "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
    "        \n",
    "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
    "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
    "        df['first_to/final'] = df['first_corner'] - df['final_corner']\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
    "        #race_type\n",
    "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n",
    "        #距離ほ10のくらいを切り捨てる\n",
    "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
    "        df.drop(['距離'], axis=1, inplace=True)\n",
    "        #インデックス名を与える\n",
    "        df.index.name = 'horse_id'\n",
    "        \n",
    "        self.horse_results = df\n",
    "        self.target_list = ['着順','賞金','着差','first_corner','final_corner','first_to_rank','first_to_final','final_to_rank']\n",
    "        \n",
    "    #n_samplesレース分馬ごとに平均する\n",
    "    def average(self, horse_id_list, date, n_sample='all'):\n",
    "        target_df = self.horse_results.query('index in @horse_id_list')\n",
    "        \n",
    "        #過去何走分取り出すか決める\n",
    "        if n_sample == 'all':\n",
    "            filtered_df = target_df[target_d['date'] < date]\n",
    "        elif n_sample > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_sample)\n",
    "        else:\n",
    "            raise Execption('n_sample must be > 0')\n",
    "            \n",
    "        \n",
    "        #集計して辞書型に入れる\n",
    "        self.average_dict = {}\n",
    "        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean\\\n",
    "            .add_suffix('_{}R'.format(n_sample))\n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            self.average_dict[column] = filered_df.groupby(['horse_id', column])\\\n",
    "                [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples))\n",
    "            \n",
    "        #6/6追加,　馬の出走感覚追加のために全レースの日付をlatestに格納\n",
    "        if n_sample == 5:\n",
    "            self.latest = filtered_df.groupby('horse_id')['date'].max().rename('latest')\n",
    "            \n",
    "    def merge(self, results, date, n_sample='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        self.average(horse_id_list, date, n_sample)\n",
    "        merged_df = df.merge(self.average_dict['non_category'], left_on='hprse_id', right_index=True, how='left')\n",
    "        \n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            merged_df = merged_df.merge(self.average_dict[column],\n",
    "                                       left_on = ['horse_id', column],\n",
    "                                       right_index = True, how='left')\n",
    "            \n",
    "        #6/6追加, 馬の出走間隔追加のために、全レースの日付を変数latestに格納\n",
    "        if n_sample == 5:\n",
    "            merged_df = merged_df.merge(self.latest, left_on = 'horse_id',\n",
    "                                       right_index=True, how='left')\n",
    "        return merged_df\n",
    "        \n",
    "    def merge_all(self, results, n_sample='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_sample) for date in tqdm(date_list)])\n",
    "        return merged_df\n",
    "    \n",
    "#開催場所をidに変換するための辞書型\n",
    "place_dict = {\n",
    "    '札幌':'01','函館':'02','福島':'03','新潟':'04','東京':'05',\n",
    "    '中山':'06','中京':'07','京都':'08','阪神':'09','小倉':'10'\n",
    "}\n",
    "    \n",
    "#レースタイプをレース結果とデータ整合させるための辞書\n",
    "race_type_dict = {\n",
    "    '芝':'芝','ダ':'ダート','障':'障害'\n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラアス内にhorse_resultsに、前処理された過去成績データが入る\n",
    "下のはpickleではないものの呼び出し　だから使えない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HorseResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tm/fn98hw1541vg9yshhxdqfgc80000gn/T/ipykernel_2066/4282514279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHorseResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorse_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhorse_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HorseResults' is not defined"
     ]
    }
   ],
   "source": [
    "#使わない\n",
    "hr = HorseResults(horse_results)\n",
    "hr.horse_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存してあるpickleデータからオブジェクトを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>着順</th>\n",
       "      <th>賞金</th>\n",
       "      <th>着差</th>\n",
       "      <th>通過</th>\n",
       "      <th>開催</th>\n",
       "      <th>date</th>\n",
       "      <th>first_corner</th>\n",
       "      <th>final_corner</th>\n",
       "      <th>final_to_rank</th>\n",
       "      <th>first_to_rank</th>\n",
       "      <th>first_to/final</th>\n",
       "      <th>race_type</th>\n",
       "      <th>course_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3-3-3-2</td>\n",
       "      <td>03</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1-1-1-1</td>\n",
       "      <td>06</td>\n",
       "      <td>2020-12-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7-6-4-4</td>\n",
       "      <td>01</td>\n",
       "      <td>2020-08-02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10-7-7-2</td>\n",
       "      <td>01</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1-1-1-1</td>\n",
       "      <td>01</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5-5-6-7</td>\n",
       "      <td>09</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>ダート</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>8-8-10-9</td>\n",
       "      <td>07</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ダート</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1-1-1-1</td>\n",
       "      <td>08</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ダート</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>5</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>10-9-9-7</td>\n",
       "      <td>08</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ダート</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4-6-7-5</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>芝</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248007 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            着順     賞金   着差        通過  開催       date  first_corner  \\\n",
       "horse_id                                                            \n",
       "2017105318  11    0.0  0.9   3-3-3-2  03 2021-07-17           3.0   \n",
       "2017105318  16    0.0  1.9   1-1-1-1  06 2020-12-26           1.0   \n",
       "2017105318   4  110.0  0.2   7-6-4-4  01 2020-08-02           7.0   \n",
       "2017105318   6    0.0  0.6  10-7-7-2  01 2019-08-31          10.0   \n",
       "2017105318   1  500.0  0.0   1-1-1-1  01 2019-07-27           1.0   \n",
       "...         ..    ...  ...       ...  ..        ...           ...   \n",
       "2017101106   7    0.0  1.6   5-5-6-7  09 2020-03-14           5.0   \n",
       "2017101106   9    0.0  1.7  8-8-10-9  07 2020-02-29           8.0   \n",
       "2017101106  13    0.0  1.8   1-1-1-1  08 2020-02-09           1.0   \n",
       "2017101106   5   51.0  1.6  10-9-9-7  08 2020-01-25          10.0   \n",
       "2017101106   8    0.0  1.6   4-6-7-5  10 2019-09-01           4.0   \n",
       "\n",
       "            final_corner  final_to_rank  first_to_rank  first_to/final  \\\n",
       "horse_id                                                                 \n",
       "2017105318           2.0           -9.0           -8.0             1.0   \n",
       "2017105318           1.0          -15.0          -15.0             0.0   \n",
       "2017105318           4.0            0.0            3.0             3.0   \n",
       "2017105318           2.0           -4.0            4.0             8.0   \n",
       "2017105318           1.0            0.0            0.0             0.0   \n",
       "...                  ...            ...            ...             ...   \n",
       "2017101106           7.0            0.0           -2.0            -2.0   \n",
       "2017101106           9.0            0.0           -1.0            -1.0   \n",
       "2017101106           1.0          -12.0          -12.0             0.0   \n",
       "2017101106           7.0            2.0            5.0             3.0   \n",
       "2017101106           5.0           -3.0           -4.0            -1.0   \n",
       "\n",
       "           race_type  course_len  \n",
       "horse_id                          \n",
       "2017105318         芝          20  \n",
       "2017105318         芝          22  \n",
       "2017105318         芝          20  \n",
       "2017105318         芝          18  \n",
       "2017105318         芝          18  \n",
       "...              ...         ...  \n",
       "2017101106       ダート          18  \n",
       "2017101106       ダート          19  \n",
       "2017101106       ダート          18  \n",
       "2017101106       ダート          18  \n",
       "2017101106         芝          18  \n",
       "\n",
       "[248007 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr = HorseResults.read_pickle(['horse_results.pickle'])\n",
    "hr.horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Peds:\n",
    "    def __init__(self, peds):\n",
    "        self.peds = peds\n",
    "        self.peds_e = pd.DataFrame()#after label encoding and transforming into category\n",
    "        \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = updata_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        \"\"\"\n",
    "        血統データをスクレイピングする関数\n",
    "        Parameters:\n",
    "        ----------\n",
    "        horse_id_list : list\n",
    "            馬IDのリスト\n",
    "        Returns:\n",
    "        ----------\n",
    "        peds_df : pandas.DataFrame\n",
    "            全血統データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "        \n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "                df = pd.read_html(url)[0]\n",
    "                \n",
    "                #重複を削除して一列のseries型に直す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "                ped = pd.concat([generations[i] for i in range[5]]).rename(horse_id)\n",
    "                \n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "        #列名をpeds_0, peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
    "        \n",
    "        return peds_df\n",
    "    \n",
    "    def encode(self):\n",
    "        df = self.peds.copy()\n",
    "        for column in df.columns:\n",
    "            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "        self.peds_e = df.astype('category')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peds_0</th>\n",
       "      <th>peds_1</th>\n",
       "      <th>peds_2</th>\n",
       "      <th>peds_3</th>\n",
       "      <th>peds_4</th>\n",
       "      <th>peds_5</th>\n",
       "      <th>peds_6</th>\n",
       "      <th>peds_7</th>\n",
       "      <th>peds_8</th>\n",
       "      <th>peds_9</th>\n",
       "      <th>...</th>\n",
       "      <th>peds_52</th>\n",
       "      <th>peds_53</th>\n",
       "      <th>peds_54</th>\n",
       "      <th>peds_55</th>\n",
       "      <th>peds_56</th>\n",
       "      <th>peds_57</th>\n",
       "      <th>peds_58</th>\n",
       "      <th>peds_59</th>\n",
       "      <th>peds_60</th>\n",
       "      <th>peds_61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017105318</th>\n",
       "      <td>503</td>\n",
       "      <td>2272</td>\n",
       "      <td>180</td>\n",
       "      <td>372</td>\n",
       "      <td>179</td>\n",
       "      <td>377</td>\n",
       "      <td>98</td>\n",
       "      <td>189</td>\n",
       "      <td>94</td>\n",
       "      <td>169</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>604</td>\n",
       "      <td>118</td>\n",
       "      <td>325</td>\n",
       "      <td>18</td>\n",
       "      <td>188</td>\n",
       "      <td>166</td>\n",
       "      <td>870</td>\n",
       "      <td>818</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017104612</th>\n",
       "      <td>222</td>\n",
       "      <td>5585</td>\n",
       "      <td>156</td>\n",
       "      <td>303</td>\n",
       "      <td>752</td>\n",
       "      <td>4633</td>\n",
       "      <td>49</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>319</td>\n",
       "      <td>...</td>\n",
       "      <td>156</td>\n",
       "      <td>367</td>\n",
       "      <td>135</td>\n",
       "      <td>122</td>\n",
       "      <td>455</td>\n",
       "      <td>286</td>\n",
       "      <td>267</td>\n",
       "      <td>383</td>\n",
       "      <td>368</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017103879</th>\n",
       "      <td>236</td>\n",
       "      <td>5206</td>\n",
       "      <td>75</td>\n",
       "      <td>23</td>\n",
       "      <td>492</td>\n",
       "      <td>3810</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>655</td>\n",
       "      <td>73</td>\n",
       "      <td>107</td>\n",
       "      <td>14</td>\n",
       "      <td>315</td>\n",
       "      <td>46</td>\n",
       "      <td>645</td>\n",
       "      <td>344</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017106259</th>\n",
       "      <td>259</td>\n",
       "      <td>1403</td>\n",
       "      <td>185</td>\n",
       "      <td>365</td>\n",
       "      <td>783</td>\n",
       "      <td>2044</td>\n",
       "      <td>21</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>579</td>\n",
       "      <td>116</td>\n",
       "      <td>110</td>\n",
       "      <td>369</td>\n",
       "      <td>784</td>\n",
       "      <td>14</td>\n",
       "      <td>1166</td>\n",
       "      <td>1264</td>\n",
       "      <td>2934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017104140</th>\n",
       "      <td>235</td>\n",
       "      <td>7267</td>\n",
       "      <td>163</td>\n",
       "      <td>366</td>\n",
       "      <td>394</td>\n",
       "      <td>1048</td>\n",
       "      <td>98</td>\n",
       "      <td>169</td>\n",
       "      <td>47</td>\n",
       "      <td>194</td>\n",
       "      <td>...</td>\n",
       "      <td>145</td>\n",
       "      <td>235</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>384</td>\n",
       "      <td>410</td>\n",
       "      <td>380</td>\n",
       "      <td>11</td>\n",
       "      <td>667</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017102659</th>\n",
       "      <td>444</td>\n",
       "      <td>1584</td>\n",
       "      <td>189</td>\n",
       "      <td>123</td>\n",
       "      <td>565</td>\n",
       "      <td>3266</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>97</td>\n",
       "      <td>236</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>564</td>\n",
       "      <td>24</td>\n",
       "      <td>280</td>\n",
       "      <td>310</td>\n",
       "      <td>253</td>\n",
       "      <td>48</td>\n",
       "      <td>928</td>\n",
       "      <td>445</td>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016102166</th>\n",
       "      <td>272</td>\n",
       "      <td>3585</td>\n",
       "      <td>153</td>\n",
       "      <td>337</td>\n",
       "      <td>493</td>\n",
       "      <td>3315</td>\n",
       "      <td>38</td>\n",
       "      <td>152</td>\n",
       "      <td>112</td>\n",
       "      <td>212</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>510</td>\n",
       "      <td>118</td>\n",
       "      <td>242</td>\n",
       "      <td>225</td>\n",
       "      <td>949</td>\n",
       "      <td>446</td>\n",
       "      <td>1118</td>\n",
       "      <td>1189</td>\n",
       "      <td>2608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016103450</th>\n",
       "      <td>469</td>\n",
       "      <td>4390</td>\n",
       "      <td>189</td>\n",
       "      <td>133</td>\n",
       "      <td>658</td>\n",
       "      <td>4848</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>192</td>\n",
       "      <td>293</td>\n",
       "      <td>...</td>\n",
       "      <td>341</td>\n",
       "      <td>118</td>\n",
       "      <td>138</td>\n",
       "      <td>297</td>\n",
       "      <td>70</td>\n",
       "      <td>352</td>\n",
       "      <td>267</td>\n",
       "      <td>329</td>\n",
       "      <td>685</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017105662</th>\n",
       "      <td>232</td>\n",
       "      <td>7326</td>\n",
       "      <td>163</td>\n",
       "      <td>260</td>\n",
       "      <td>531</td>\n",
       "      <td>4431</td>\n",
       "      <td>98</td>\n",
       "      <td>169</td>\n",
       "      <td>233</td>\n",
       "      <td>312</td>\n",
       "      <td>...</td>\n",
       "      <td>344</td>\n",
       "      <td>277</td>\n",
       "      <td>150</td>\n",
       "      <td>400</td>\n",
       "      <td>171</td>\n",
       "      <td>55</td>\n",
       "      <td>239</td>\n",
       "      <td>203</td>\n",
       "      <td>151</td>\n",
       "      <td>2113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017101106</th>\n",
       "      <td>252</td>\n",
       "      <td>1546</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>639</td>\n",
       "      <td>1886</td>\n",
       "      <td>55</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>428</td>\n",
       "      <td>707</td>\n",
       "      <td>47</td>\n",
       "      <td>116</td>\n",
       "      <td>75</td>\n",
       "      <td>464</td>\n",
       "      <td>75</td>\n",
       "      <td>555</td>\n",
       "      <td>222</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11557 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           peds_0 peds_1 peds_2 peds_3 peds_4 peds_5 peds_6 peds_7 peds_8  \\\n",
       "2017105318    503   2272    180    372    179    377     98    189     94   \n",
       "2017104612    222   5585    156    303    752   4633     49    140    203   \n",
       "2017103879    236   5206     75     23    492   3810      0    105     51   \n",
       "2017106259    259   1403    185    365    783   2044     21     88     37   \n",
       "2017104140    235   7267    163    366    394   1048     98    169     47   \n",
       "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2017102659    444   1584    189    123    565   3266     78     68     97   \n",
       "2016102166    272   3585    153    337    493   3315     38    152    112   \n",
       "2016103450    469   4390    189    133    658   4848     78     68    192   \n",
       "2017105662    232   7326    163    260    531   4431     98    169    233   \n",
       "2017101106    252   1546     63      6    639   1886     55     85     86   \n",
       "\n",
       "           peds_9  ... peds_52 peds_53 peds_54 peds_55 peds_56 peds_57  \\\n",
       "2017105318    169  ...      51     604     118     325      18     188   \n",
       "2017104612    319  ...     156     367     135     122     455     286   \n",
       "2017103879     25  ...     445     655      73     107      14     315   \n",
       "2017106259     71  ...     188     579     116     110     369     784   \n",
       "2017104140    194  ...     145     235      10      39     384     410   \n",
       "...           ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "2017102659    236  ...     313     564      24     280     310     253   \n",
       "2016102166    212  ...       5     510     118     242     225     949   \n",
       "2016103450    293  ...     341     118     138     297      70     352   \n",
       "2017105662    312  ...     344     277     150     400     171      55   \n",
       "2017101106      7  ...     428     707      47     116      75     464   \n",
       "\n",
       "           peds_58 peds_59 peds_60 peds_61  \n",
       "2017105318     166     870     818     511  \n",
       "2017104612     267     383     368    1795  \n",
       "2017103879      46     645     344    1880  \n",
       "2017106259      14    1166    1264    2934  \n",
       "2017104140     380      11     667    2330  \n",
       "...            ...     ...     ...     ...  \n",
       "2017102659      48     928     445    2262  \n",
       "2016102166     446    1118    1189    2608  \n",
       "2016103450     267     329     685    1326  \n",
       "2017105662     239     203     151    2113  \n",
       "2017101106      75     555     222     456  \n",
       "\n",
       "[11557 rows x 62 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Peds.read_pickle(['peds.pickle'])\n",
    "p.encode()\n",
    "p.peds_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'性齡'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '性齡'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tm/fn98hw1541vg9yshhxdqfgc80000gn/T/ipykernel_2066/2247014657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results.pickle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tm/fn98hw1541vg9yshhxdqfgc80000gn/T/ipykernel_2066/2775212210.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#性年を性と年齢に分ける\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"性\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'性齡'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'年齢'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'性齡'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3455\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '性齡'"
     ]
    }
   ],
   "source": [
    "r = Results.read_pickle(['results.pickle'])\n",
    "r.preprocessing()\n",
    "r.data_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
